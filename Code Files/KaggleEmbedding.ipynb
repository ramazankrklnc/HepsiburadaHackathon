{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-23T10:31:58.299831Z",
     "iopub.status.busy": "2025-08-23T10:31:58.299583Z",
     "iopub.status.idle": "2025-08-23T10:31:58.534751Z",
     "shell.execute_reply": "2025-08-23T10:31:58.533830Z",
     "shell.execute_reply.started": "2025-08-23T10:31:58.299813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 23 10:31:58 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   43C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "/kaggle/input/hepsiburadadata/sample_submission.csv\n",
      "/kaggle/input/hepsiburadadata/train.csv\n",
      "/kaggle/input/hepsiburadadata/test.csv\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import os\n",
    "for r,_,fs in os.walk(\"/kaggle/input\"):\n",
    "    for f in fs:\n",
    "        print(os.path.join(r,f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T10:31:58.536107Z",
     "iopub.status.busy": "2025-08-23T10:31:58.535804Z",
     "iopub.status.idle": "2025-08-23T10:32:03.831187Z",
     "shell.execute_reply": "2025-08-23T10:32:03.830099Z",
     "shell.execute_reply.started": "2025-08-23T10:31:58.536083Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Gerekli kütüphanelerin kurulumu\n",
    "!pip install -q transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T10:32:03.834187Z",
     "iopub.status.busy": "2025-08-23T10:32:03.833574Z",
     "iopub.status.idle": "2025-08-23T10:32:19.832473Z",
     "shell.execute_reply": "2025-08-23T10:32:19.831660Z",
     "shell.execute_reply.started": "2025-08-23T10:32:03.834158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================= HEPSİBURADA KAGGLE PIPELINE (OPTIMIZED v4 - SINGLE GPU STABLE) =================\n",
    "import os, re, unicodedata, gc, math, time, warnings, json\n",
    "import numpy as np, pandas as pd, torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "from contextlib import contextmanager\n",
    "from tqdm.auto import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ------------------- CONFIG -------------------\n",
    "CFG = {\n",
    "    # Genel\n",
    "    'REQUIRE_GPU': True,\n",
    "    'SEED': 42,\n",
    "    'DEBUG': False,  # Hızlı debug için az veri kullan\n",
    "    \n",
    "    # Veri Yolları (Kaggle)\n",
    "    'DATA_DIR': '/kaggle/input/hepsiburadadata',\n",
    "    'OUTPUT_DIR': '/kaggle/working',\n",
    "    'TEXT_FALLBACK_COLS': [\"clean_address\", \"address\", \"text\"],\n",
    "    'LABEL_COL': \"label\",\n",
    "    \n",
    "    # Model & Training\n",
    "    'MODELS': [\n",
    "        \"intfloat/multilingual-e5-large\",  # Önce sadece ana model\n",
    "    ],\n",
    "    'NUM_FOLDS': 5,                        # Cross-validation\n",
    "    'VAL_SIZE': 0.10,                      # Her fold için\n",
    "    \n",
    "    # Embedding (Optimize edilmiş)\n",
    "    'EMB_BATCH': 16,                       # Daha küçük batch\n",
    "    'EMB_MAXLEN': 96,                      # Daha kısa sequence\n",
    "    'SAVE_FP16': True,                     # Disk tasarrufu\n",
    "    \n",
    "    # MLP (Optimize edilmiş)\n",
    "    'H1': 1024, 'H2': 512, 'H3': 256,     # Daha küçük model\n",
    "    'DROPOUT1': 0.35, 'DROPOUT2': 0.25, 'DROPOUT3': 0.15,\n",
    "    'LR': 2e-4,\n",
    "    'WARMUP_RATIO': 0.05,                  # Warmup\n",
    "    'WD': 0.01,                            # L2 reg\n",
    "    'EPOCHS': 15,                          # Daha az epoch\n",
    "    'BATCH_TRAIN': 8,                      # Daha küçük batch\n",
    "    'LABEL_SMOOTH': 0.1,                   # Regularization\n",
    "    'PATIENCE': 3,                         # Early stopping\n",
    "    'CLIP_NORM': 1.0,                      # Gradient clipping\n",
    "    \n",
    "    # Ensemble & Predict\n",
    "    'FOLD_WEIGHTS': None,                  # Auto-weighted\n",
    "    'FINETUNE_STEPS': 2,                   # Son fine-tune\n",
    "    'BATCH_PRED': 32,                      # Prediction batch\n",
    "    \n",
    "    # Log/ilerleme\n",
    "    'SHOW_PROGRESS': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T10:32:19.833925Z",
     "iopub.status.busy": "2025-08-23T10:32:19.833405Z",
     "iopub.status.idle": "2025-08-23T10:32:23.361248Z",
     "shell.execute_reply": "2025-08-23T10:32:23.360421Z",
     "shell.execute_reply.started": "2025-08-23T10:32:19.833895Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU Count: 2\n",
      "GPU 0: Tesla T4\n",
      "GPU 1: Tesla T4\n",
      "\n",
      "Train shape: (848237, 2) Test shape: (217241, 2)\n",
      "Columns: ['address', 'label']\n"
     ]
    }
   ],
   "source": [
    "# ------------------- ENV & DEVICE -------------------\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "torch.set_num_threads(1)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "np.random.seed(CFG[\"SEED\"])\n",
    "torch.manual_seed(CFG[\"SEED\"])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(CFG[\"SEED\"])\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if CFG[\"REQUIRE_GPU\"]:\n",
    "    assert device == \"cuda\", \"GPU görünmüyor. Lütfen GPU runtime aç ve tekrar çalıştır.\"\n",
    "print(\"Device:\", device)\n",
    "if device == \"cuda\":\n",
    "    print(\"GPU Count:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}:\", torch.cuda.get_device_name(i))\n",
    "\n",
    "# ------------------- LOAD DATA -------------------\n",
    "TRAIN_CSV = os.path.join(CFG[\"DATA_DIR\"], \"train.csv\")\n",
    "TEST_CSV  = os.path.join(CFG[\"DATA_DIR\"], \"test.csv\")\n",
    "SAMPLE_SUB = os.path.join(CFG[\"DATA_DIR\"], \"sample_submission.csv\")\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "test  = pd.read_csv(TEST_CSV)\n",
    "sample = pd.read_csv(SAMPLE_SUB)\n",
    "\n",
    "if CFG[\"DEBUG\"]:\n",
    "    print(\"DEBUG MODE: Az veri kullanılıyor!\")\n",
    "    train = train.sample(1000, random_state=CFG[\"SEED\"]).reset_index(drop=True)\n",
    "    test = test.sample(1000, random_state=CFG[\"SEED\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nTrain shape:\", train.shape, \"Test shape:\", test.shape)\n",
    "print(\"Columns:\", train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T10:32:23.362854Z",
     "iopub.status.busy": "2025-08-23T10:32:23.362232Z",
     "iopub.status.idle": "2025-08-23T10:32:57.256118Z",
     "shell.execute_reply": "2025-08-23T10:32:57.255394Z",
     "shell.execute_reply.started": "2025-08-23T10:32:23.362818Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'address' üzerinden gelişmiş temizlik yapılıyor -> 'clean_address'\n",
      "\n",
      "Unique labels: 10390\n"
     ]
    }
   ],
   "source": [
    "# ------------------- ADVANCED TEXT PREPROCESSING -------------------\n",
    "# Temel temizlik\n",
    "turkish_map = str.maketrans({\n",
    "    \"ç\":\"c\", \"ğ\":\"g\", \"ı\":\"i\", \"ö\":\"o\", \"ş\":\"s\", \"ü\":\"u\",\n",
    "    \"Ç\":\"c\", \"Ğ\":\"g\", \"İ\":\"i\", \"Ö\":\"o\", \"Ş\":\"s\", \"Ü\":\"u\"\n",
    "})\n",
    "\n",
    "# Genişletilmiş kısaltma sözlüğü\n",
    "abbrev_map = {\n",
    "    # Temel adres\n",
    "    \"mah\":\"mahallesi\", \"mah.\":\"mahallesi\", \"mh\":\"mahallesi\", \"mh.\":\"mahallesi\",\n",
    "    \"sok\":\"sokak\", \"sok.\":\"sokak\", \"sk\":\"sokak\", \"sk.\":\"sokak\",\n",
    "    \"cad\":\"cadde\", \"cad.\":\"cadde\", \"caddesi\":\"cadde\", \"cd\":\"cadde\", \"cd.\":\"cadde\",\n",
    "    \"bulvari\":\"bulvar\", \"bulvarı\":\"bulvar\", \"bulv\":\"bulvar\", \"blv\":\"bulvar\",\n",
    "    \"apt\":\"apartman\", \"apt.\":\"apartman\", \"ap\":\"apartman\", \"ap.\":\"apartman\",\n",
    "    # Numaralar\n",
    "    \"no\":\"numara\", \"no.\":\"numara\", \"no::\":\"numara\", \"no:\":\"numara\",\n",
    "    # Bina detayları\n",
    "    \"daire\":\"daire\", \"d\":\"daire\", \"d.\":\"daire\", \"d:\":\"daire\",\n",
    "    \"kat\":\"kat\", \"k\":\"kat\", \"k.\":\"kat\", \"k:\":\"kat\",\n",
    "    \"blok\":\"blok\", \"bl\":\"blok\", \"bl.\":\"blok\", \"b\":\"blok\", \"b.\":\"blok\",\n",
    "    # Site/kompleks\n",
    "    \"sitesi\":\"site\", \"st.\":\"site\", \"evleri\":\"evler\", \"konutlari\":\"konutlar\",\n",
    "    # İş yeri\n",
    "    \"is\":\"iş\", \"mrk\":\"merkez\", \"mrk.\":\"merkez\", \"merkezi\":\"merkez\",\n",
    "    \"org.san.\":\"organize sanayi\", \"osb\":\"organize sanayi\", \"san.\":\"sanayi\",\n",
    "    # Bölge\n",
    "    \"böl\":\"bölge\", \"böl.\":\"bölge\", \"bolge\":\"bölge\", \"bolg.\":\"bölge\",\n",
    "    # Yön\n",
    "    \"kuz\":\"kuzey\", \"kuz.\":\"kuzey\", \"gun\":\"güney\", \"gün\":\"güney\",\n",
    "    \"dog\":\"doğu\", \"doğ\":\"doğu\", \"bat\":\"batı\", \"bat.\":\"batı\",\n",
    "}\n",
    "\n",
    "# Regex patterns\n",
    "_re_nonword = re.compile(r\"[^\\w\\s]\")\n",
    "_re_letter_digit = re.compile(r\"([a-z])(\\d)\")\n",
    "_re_digit_letter = re.compile(r\"(\\d)([a-z])\")\n",
    "_re_manydigits = re.compile(r\"\\d{5,}\")\n",
    "_re_manypunct = re.compile(r\"[^\\w\\s]{2,}\")\n",
    "_re_space = re.compile(r\"\\s+\")\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKD\", str(s))\n",
    "    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "\n",
    "def clean_address(addr: str) -> str:\n",
    "    # Temel normalizasyon\n",
    "    addr = strip_accents(addr).lower().translate(turkish_map)\n",
    "    \n",
    "    # Noktalama ve boşluk temizliği\n",
    "    addr = _re_nonword.sub(\" \", addr)\n",
    "    addr = _re_manypunct.sub(\" \", addr)\n",
    "    \n",
    "    # Sayı-harf arası boşluk\n",
    "    addr = _re_letter_digit.sub(r\"\\1 \\2\", addr)\n",
    "    addr = _re_digit_letter.sub(r\"\\1 \\2\", addr)\n",
    "    \n",
    "    # Uzun sayıları standartlaştır\n",
    "    addr = _re_manydigits.sub(\" NUM \", addr)\n",
    "    \n",
    "    # Boşlukları normalize et\n",
    "    addr = _re_space.sub(\" \", addr).strip()\n",
    "    \n",
    "    # Kısaltmaları genişlet\n",
    "    toks = []\n",
    "    for t in addr.split():\n",
    "        # Nokta ile biten kelimeler için de kontrol\n",
    "        t_nodot = t.rstrip(\".\")\n",
    "        if t_nodot in abbrev_map:\n",
    "            toks.append(abbrev_map[t_nodot])\n",
    "        else:\n",
    "            toks.append(t)\n",
    "    \n",
    "    return \" \".join(toks)\n",
    "\n",
    "# Adres metni için kolon seç\n",
    "TEXT_COL = None\n",
    "for col in CFG[\"TEXT_FALLBACK_COLS\"]:\n",
    "    if col in train.columns:\n",
    "        TEXT_COL = col\n",
    "        break\n",
    "if TEXT_COL is None:\n",
    "    raise ValueError(\"Adres metni için bir kolon bulunamadı. TEXT_FALLBACK_COLS'a göre (clean_address/address/text) bekleniyordu.\")\n",
    "\n",
    "# Temizlenmiş adres kolonu oluştur/güncelle\n",
    "if TEXT_COL != \"clean_address\":\n",
    "    print(f\"'{TEXT_COL}' üzerinden gelişmiş temizlik yapılıyor -> 'clean_address'\")\n",
    "    train[\"clean_address\"] = train[TEXT_COL].astype(str).apply(clean_address)\n",
    "    test[\"clean_address\"]  = test[TEXT_COL].astype(str).apply(clean_address)\n",
    "    TEXT_COL = \"clean_address\"\n",
    "\n",
    "LABEL_COL = CFG[\"LABEL_COL\"]\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "y_all = le.fit_transform(train[LABEL_COL].astype(str).values)\n",
    "print(\"\\nUnique labels:\", len(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T10:32:57.260238Z",
     "iopub.status.busy": "2025-08-23T10:32:57.259683Z",
     "iopub.status.idle": "2025-08-23T13:03:56.445670Z",
     "shell.execute_reply": "2025-08-23T13:03:56.444874Z",
     "shell.execute_reply.started": "2025-08-23T10:32:57.260217Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting embedding computation...\n",
      "Using 2 GPUs\n",
      "\n",
      "Processing model: intfloat/multilingual-e5-large\n",
      "Loading model: intfloat/multilingual-e5-large\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec84b0aac92b4e31ae51e696c11ef621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1156399f7086449997e7cf48ab9560f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a57ddd9f0c14b9ebcaf496d899fc0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3fab2249f14c5a99441b1c7ec80bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e3e61e637f4615bd5fb3fde8749f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 10:33:17.503849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755945197.830846      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755945197.923422      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3706be6aa3694849a295d2281676bca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n",
      "Creating embeddings -> /kaggle/working/X_train_multilingual_e5_large.npy\n",
      "Increased batch size to 32 for multi-GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b74e5ee86ee4fb1b7722290830ebff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding (xlm-roberta):   0%|          | 0/26508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape: (848237, 1024)\n",
      "Creating embeddings -> /kaggle/working/X_test_multilingual_e5_large.npy\n",
      "Increased batch size to 32 for multi-GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257a5f65fd054227bdd2a49bf15de9a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding (xlm-roberta):   0%|          | 0/6789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test embeddings shape: (217241, 1024)\n",
      "\n",
      "Embedding shapes:\n",
      "- multilingual-e5-large: (848237, 1024)\n",
      "\n",
      "GPU 0 Memory Usage:\n",
      "Allocated: 0.01 GB\n",
      "Cached: 0.01 GB\n",
      "\n",
      "GPU 1 Memory Usage:\n",
      "Allocated: 0.01 GB\n",
      "Cached: 0.02 GB\n"
     ]
    }
   ],
   "source": [
    "# ------------------- EMBEDDING MODEL -------------------\n",
    "@contextmanager\n",
    "def _tok_parallel(on=True):\n",
    "    \"\"\"Tokenizer paralelleştirme yönetimi\"\"\"\n",
    "    old = os.environ.get(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "    try:\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\" if on else \"false\"\n",
    "        yield\n",
    "    finally:\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = old\n",
    "\n",
    "class FastEmbeddingModel:\n",
    "    def __init__(self, model_name, device=\"cuda\"):\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Model'i DataParallel ile çift GPU'ya dağıt\n",
    "        base_model = AutoModel.from_pretrained(model_name)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "            self.model = torch.nn.DataParallel(base_model).to(device)\n",
    "        else:\n",
    "            self.model = base_model.to(device)\n",
    "            \n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Model boyutuna göre attention weights\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        self.attention_weights = torch.nn.Parameter(\n",
    "            torch.ones(hidden_size, device=device)\n",
    "        ).softmax(dim=0)\n",
    "    \n",
    "    def weighted_pool(self, last_hidden, attn_mask):\n",
    "        mask = attn_mask.unsqueeze(-1).float()\n",
    "        weighted = last_hidden * self.attention_weights\n",
    "        return (weighted * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def embed(self, texts, batch=32, max_len=128, show_progress=True):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Çift GPU varsa batch size'ı 2 katına çıkar\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            batch = batch * 2\n",
    "            print(f\"Increased batch size to {batch} for multi-GPU\")\n",
    "        \n",
    "        # Uzunluğa göre sırala ve batch'le\n",
    "        lengths = np.array([len(t) for t in texts], dtype=np.int32)\n",
    "        order = np.argsort(lengths)\n",
    "        restore = np.empty_like(order); restore[order] = np.arange(len(order))\n",
    "        texts_sorted = [texts[i] for i in order]\n",
    "        \n",
    "        vecs = []\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Paralel tokenization\n",
    "        with _tok_parallel(True):\n",
    "            # Tüm metinleri tokenize et\n",
    "            encoded = self.tokenizer(\n",
    "                texts_sorted,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Batch'ler halinde işle\n",
    "            for i in tqdm(range(0, len(texts_sorted), batch), \n",
    "                         disable=not show_progress,\n",
    "                         desc=f\"Embedding ({self.model.module.config.model_type if hasattr(self.model, 'module') else self.model.config.model_type})\"):\n",
    "                \n",
    "                inputs = {\n",
    "                    k: v[i:i+batch].to(self.device) \n",
    "                    for k, v in encoded.items()\n",
    "                }\n",
    "                \n",
    "                # Mixed precision\n",
    "                with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                    outputs = self.model(**inputs)\n",
    "                    pooled = self.weighted_pool(\n",
    "                        outputs.last_hidden_state,\n",
    "                        inputs[\"attention_mask\"]\n",
    "                    )\n",
    "                \n",
    "                vecs.append(pooled.cpu())\n",
    "                \n",
    "                # Bellek temizliği\n",
    "                del inputs, outputs, pooled\n",
    "                if i % 100 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "        \n",
    "        X = torch.cat(vecs, dim=0).numpy()\n",
    "        X = X[restore]\n",
    "        \n",
    "        # L2 normalize\n",
    "        X /= np.linalg.norm(X, axis=1, keepdims=True).clip(1e-12)\n",
    "        return X\n",
    "\n",
    "def fast_embeddings(texts, model, fname, batch=32, max_len=128):\n",
    "    fname = os.path.join(CFG[\"OUTPUT_DIR\"], fname)\n",
    "    if os.path.exists(fname):\n",
    "        print(f\"Loading cached embeddings: {fname}\")\n",
    "        return np.load(fname, mmap_mode=\"r\")\n",
    "    \n",
    "    print(f\"Creating embeddings -> {fname}\")\n",
    "    X = model.embed(texts, batch=batch, max_len=max_len)\n",
    "    np.save(fname, X.astype(np.float16))\n",
    "    return np.load(fname, mmap_mode=\"r\")\n",
    "\n",
    "# Ana embedding hesaplama\n",
    "print(\"Starting embedding computation...\")\n",
    "print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "embeddings_train, embeddings_test = [], []\n",
    "\n",
    "for model_name in CFG[\"MODELS\"]:\n",
    "    safe_model = model_name.split(\"/\")[-1].replace(\"-\", \"_\")\n",
    "    print(f\"\\nProcessing model: {model_name}\")\n",
    "    \n",
    "    model = FastEmbeddingModel(model_name, device)\n",
    "    \n",
    "    # Train embeddings\n",
    "    X_train = fast_embeddings(\n",
    "        train[TEXT_COL].astype(str).tolist(),\n",
    "        model,\n",
    "        f\"X_train_{safe_model}.npy\",\n",
    "        batch=CFG[\"EMB_BATCH\"],\n",
    "        max_len=CFG[\"EMB_MAXLEN\"]\n",
    "    )\n",
    "    embeddings_train.append(X_train)\n",
    "    print(f\"Train embeddings shape: {X_train.shape}\")\n",
    "    \n",
    "    # Test embeddings\n",
    "    X_test = fast_embeddings(\n",
    "        test[TEXT_COL].astype(str).tolist(),\n",
    "        model,\n",
    "        f\"X_test_{safe_model}.npy\",\n",
    "        batch=CFG[\"EMB_BATCH\"],\n",
    "        max_len=CFG[\"EMB_MAXLEN\"]\n",
    "    )\n",
    "    embeddings_test.append(X_test)\n",
    "    print(f\"Test embeddings shape: {X_test.shape}\")\n",
    "    \n",
    "    # Bellek temizliği\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    time.sleep(2)  # GPU soğuma\n",
    "\n",
    "print(\"\\nEmbedding shapes:\")\n",
    "for i, model_name in enumerate(CFG[\"MODELS\"]):\n",
    "    print(f\"- {model_name.split('/')[-1]}: {embeddings_train[i].shape}\")\n",
    "\n",
    "# GPU kullanım bilgisi\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nGPU {i} Memory Usage:\")\n",
    "        print(f\"Allocated: {torch.cuda.memory_allocated(i)/1024**3:.2f} GB\")\n",
    "        print(f\"Cached: {torch.cuda.memory_reserved(i)/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8119109,
     "sourceId": 12837475,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
